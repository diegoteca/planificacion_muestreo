---
title: "Azar simple"
---

Como se trata de un formato tipo taller a partir de ahora vamos a ir interactuando entre algo de teoría, algo de datos y algo de código. Por ahora vamos a trabajar con un sólo insumo empírico que es una base de datos de establecimientos educativos de nivel primario, de gestión estatal, de la provincia de Buenos Aires y el código con que vamos a procesar los datos va a ser en lenguaje R. Por otro lado, con este insumo nos vamos a focalizar en el *diseño* de las muestras y en la construcción de ponderadores y expansores pero no vamos a ver por ahora estrategias de calibración. En otras palabras, vamos a ver estrategias *ex-ante* la producción del dato primario y no vamos a ver estrategias, usualmente más realistas pero también más variables de forma circunstancial (p.e. ajuste por no respuesta ), *ex-post* producción del dato primario.

## Porqué el azar produce muestras (aproximadamente) representativas

Si ser muy profundos en cuanto a la teoría del muestreo o en cuanto a su justificación más académica en lo que sigue se intenta recordar que sucede si se realizan muestras aleatorias dentro de una población. Para eso vamos a simular que hacemos muchas muestras aleatorias simples sobre una población imaginaria. Primero haremos, o más bien repetiremos, muestras relativamente pequeñas y luego haremos muestras algo más grandes.

Para tener una mejor experiencia de este simulador es aconsejable su ejecución a través de este [link](https://planificacion-muestreo.netlify.app/simulador_teorema_limite_central.html).

```{=html}
<iframe 
width = 100% 
height = "500" 
src = "https://diegoteca.com.ar/SimuladorTeoremaLimiteCentral.html" 
title = "Webpage example">
</iframe>
```
Con este programa podemos jugar de varias formas. Aquí nos interesa las siguientes. En primer lugar ver que pasa, en los términos de la figura "datos acumulativos de las muestras" cuando realizamos muchas muestras sobre una misma población. Como veremos, estas distribuciones, en especial si la muestra contiene más de 30 casos, converge hacia un patrón de distribución "normal". Lo interesante es que esta distribución emerge con mayor o menor rapidez, de manera independiente de la forma de la población. Esto se puede probar escogiendo diferente poblaciones.

Por otro lado se puede probar que sucede si, frente a una misma población (p.e. la población "Opción 3"), se realizan muestras (\>30) de diferente tamaño. Como se observará siempre se obtiene una distribución "normal" aunque con diferente varianza.

## Población

```{r}
#| label: setting

# Hay que instalar las siguientes librerias previamente sino las tienen instaladas

library(here)
library(readxl)
library(janitor)
library(tidyverse)
library(gt)
library(sampling)
library(gtsummary)

i_am("azar.qmd")

theme_gtsummary_language(
language = "es",
decimal.mark = ",",
big.mark = ".")
```

```{r}
#| label: input

base = read_xlsx(here("Inputs", "base_escuelas_primaria.xlsx")) |>
clean_names()

base |>
slice_head(n = 3) |>
gt()
```

Como puede observarse se trata de una base de escuelas en el sentido que en cada fila hay un establecimiento diferente que contiene una serie de propiedades de los mismos en cada una de las columnas. Algunas de esas propiedades se podrían considerar como intrínsecas de cada escuela (clave, región, ámbito, etc.) pero otras pueden considerarse como propiedades derivadas de los establecimientos en el sentido que devienen de agregaciones de los estudiantes de cada escuela (prueba_x) o de los directivos de los mismos (direct_x). Esta distinciones son importantes cuando se quiere realizar una muestra porque esto indica posibilidades y límites sobre a qué población se puede realizar una "buena" muestra desde este archivo. En este contexto, la información de este archivo es particularmente buena para realizar una muestra de colegios pero no tan apropiada para realizar una muestra de estudiantes o directivos...al menos si se lo compara con tener acceso a lista de estudiantes o directivos con el respectivo dato de la escuela. Por ahora trabajaremos con este archivo para realizar diferentes muestras de establecimientos educativos. Un plus pedagógico de esta estrategia es que vamos a tener siempre los valores de las estimaciones de las diferentes muestras y los respectivos valores de los parámetros poblaciones para comparar resultados. Esta situación no es la usual porque si ya se tiene el parámetro poblacional no es necesario la realización de una muestra para su estimación.

Dentro de los parámetros poblacionales vamos a calcular los siguientes:

-   Matrícula

-   Secciones

-   Sondeo primero

-   Sondeo segundo

-   Región

-   Ámbito

    Algunas de las variables anteriores son categóricas (región, ámbito) y otras no. Vamos a ver que esto importa porque no es lo mismo estimar un parámetro continuo que uno categórico. Dentro de estos últimos tampoco es lo mismo estimar una variable con 25 categorías (región) que una variable categórica de 3 categorías (ámbito).

    En la @tbl-parametros_base puede observarse algunos de los valores de estos parámetros. Para el caso de las variables continuas (Matrícula, Secciones, sondeo primero, sondeo segundo) se ha calculado la media junto con los valores del primer y tercer cuartil. En el caso de las variables categóricas (región, ámbito) se han calculado los respectivos porcentajes de cada categoría.

    ```{r}
    #| label: tbl-parametros_base
    #| tbl-cap: Valores poblacionales de las escuelas

    base_toy = base |>
    select(clave, matricula, secciones, sondeo_primero, sondeo_segundo, region, ambito) 
    pob_param = base_toy |>
    tbl_summary(include = !clave,
                statistic = list(
          all_continuous() ~ "{mean} ({p25}, {p75})"))

    pob_param
    ```

Estos parámetros "conocidos" van a tener 2 funciones en este taller. Por un lado nos van a servir para cotejar las distintas estimaciones de los diferentes diseños muestrales que vamos a usar y, por otro lado, nos van a servir, más adelante, para mejorar las muestras unitarias o individuales efectivamente obtenidas. Esto último, más allá de las tecnicas específicas que se utilicen, es importante por lo siguiente:

Las muestras que se hacen en la realidad son "únicas" o "individuales" en el sentido que no se repiten. Esto quiere decir que, si bien la teoría muchas veces supone una distribución de muestras (como en las simulaciones que se hicieron antes en donde se realizaban varias muestras de una misma población) en la realidad el investigador que sale a campo muchas veces dispone de sólo una muestra y la idea es que "esa" muestra (y no cualquier otra posible) sea de las mejores y no de las peores. En este sentido es útil tener herramientas que permitan saber si la muestra es de las buenas o de las malas y, si se trata del segundo caso, como mejorarlas.

## Azar simple

Dentro de los métodos aleatorios el método del azar simple (*random sampling*) es un método tradicional y particularmente útil desde un punto de vista pedagógico para comenzar ya que muchos otros métodos son variaciones (usualmente más complejas) del azar simple. El método del azar simple es el que intuitivamente se ha utilizado en la simulación anterior. En la actualidad se podría afirmar que se encuentra en el medio de un continuo de situaciones en cuando al grado de información exigida para su realización. Lo "único" que pide es una lista de contactos de la población que se quiere analizar. La razón por la que "único" se encuentra entre comillas es la siguiente. La necesidad de la una lista puede ser algo exigente en algunas investigaciones (p.e. poblaciones invisibles) aunque algo insuficiente para otras (p.e. diseños con muestras estratificadas). La idea de contacto es algo polisémica pero aquí apunta a que el caso seleccionado se pueda contactar de alguna forma (p.e. dirección de domicilio, mail, celular, etc.) para realizarle las observaciones/mediciones.[^azar-1]

```{r}
#| label: azar_simple

set.seed(3) 

sizes = c(100, 200, 300, 400, 500)
n = 5
sample_list <- lapply(n, function(i) base_toy[sample(nrow(base_toy), i), ])

m_azar_simple_100 = base_toy |>
slice_sample(n = 100, replace = T)

m_azar_simple_200 = base_toy |>
slice_sample(n = 200, replace = T)

m_azar_simple_300 = base_toy |>
slice_sample(n = 300, replace = T)

m_azares = base_toy |>
summary(papa = map(1:n, \(i) slice_sample(n = 100)))

m_azares_2 = lapply(replicate(2, slice_sample(base_toy, 
                                       n = 100, 
                                       replace = T)))

```

```{r}
#| label: estimaciones_azar_simple


matricula <- lapply(sample_list, pluck, 2)
mean_value <- lapply(matricula, mean, na.rm = TRUE) |> 
unlist()
mean_value

secciones <- lapply(sample_list, pluck, 3)
mean_value <- lapply(secciones, mean, na.rm = TRUE) |> 
unlist()
mean_value


```

## Muestreo sistemático

```{r}
start <- sample(1:10, size = 1)
m_azar_sistematico_100 = base |>
  slice(seq(start, n(), by = ))
```

[^azar-1]: Más adelante veremos que el muestreo sistemático puede realizarse aún sin la presencia de una "lista" previa a la cual tengan acceso los diseñadores de la muestra aunque sí los que finalmente la ejecuten. En efecto, es compatible con reglas del tipo "Una vez con los potenciales casos a disposición eliga al primer caso, deje pasar 5 casos y seleccione al siguiente, vuelva a dejar 5 casos y elija al siguiente hasta llegar a seleccionar x casos ".
